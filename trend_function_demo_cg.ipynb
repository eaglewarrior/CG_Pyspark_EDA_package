{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import isnan\n",
    "import pyspark\n",
    "import pyspark.sql.functions as f\n",
    "import matplotlib as mpl\n",
    "from pyspark.sql.functions import window\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "# Spark datatypes to contruct schema\n",
    "from pyspark.sql.types import  (StructType, \n",
    "                                StructField, \n",
    "                                DateType, \n",
    "                                BooleanType,\n",
    "                                DoubleType,\n",
    "                                IntegerType,\n",
    "                                StringType,\n",
    "                               TimestampType)\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import avg \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Spark session necessary to do before starting any spark code\n",
    "spark = SparkSession.builder.appName(\"CG_analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema=StructType([StructField(\"customer_id\", StringType(), True),\n",
    "                            StructField(\"response\", IntegerType(), True),\n",
    "                            StructField(\"trans_date\", StringType(), True ),\n",
    "                            StructField(\"tran_amount\", DoubleType(), True)\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"transaction_time_trend.csv\",\n",
    "                       header = True, \n",
    "                        schema = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(customer_id='CS1112', response=0, trans_date='14-Jan-15', tran_amount=39.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date column should be in format yyyy-mm-dd: e.g 2013-09-24 before using this function\n",
    "## write a userdefined function to transform this date\n",
    "## here I have date format trans_date='14-Jan-15'I will transform it to 2015-01-14 using this udf or userdefined func\n",
    "def date_parse(date):\n",
    "    if date is not None:\n",
    "        sp=date.split('-')\n",
    "        month={\n",
    "        'Jan' : '01',\n",
    "        'Feb' : '02',\n",
    "        'Mar' : '03',\n",
    "        'Apr' : '04',\n",
    "        'May' : '05',\n",
    "        'Jun' : '06',\n",
    "        'Jul' : '07',\n",
    "        'Aug' : '08',\n",
    "        'Sep' : '09', \n",
    "        'Oct' : '10',\n",
    "        'Nov' : '11',\n",
    "        'Dec' : '12'}\n",
    "        print(month[sp[1]])\n",
    "        return '20'+sp[2]+'-'+month[sp[1]]+'-'+sp[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiating this udf to a variable to be use it\n",
    "parse=udf(lambda s: date_parse(s), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gathering the required columns to be passed in function\n",
    "new_data=data.select(\"response\",\"tran_amount\",parse(\"trans_date\").alias(\"parsed_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(response=0, tran_amount=39.0, parsed_date='2015-01-14')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_op_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time(datetime_str):\n",
    "    return datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trend_df(data,date_col_name,tran_amountcol_name,agg_type=\"sum\",frequency_window='1 day',key2_groupby=False,key2=None,frequency_period='D',to_csvopt=False):\n",
    "    ## Here depending on if user wants groupby with some specific key or specific aggregation 4 cases are written\n",
    "    if agg_type==\"avg\"and key2_groupby==True:\n",
    "        wingrp_result=data.groupBy(key2,window(date_col_name, frequency_window)).agg(avg(tran_amountcol_name).alias(\"avg\"))\n",
    "        result_win=wingrp_result.select(key2,wingrp_result.window.start.cast(\"string\").alias(\"start\"),wingrp_result.window.end.cast(\"string\").alias(\"end\"), agg_type).toPandas()\n",
    "    if agg_type==\"sum\"and key2_groupby==True:\n",
    "        wingrp_result=data.groupBy(key2,window(date_col_name, frequency_window)).agg(_sum(tran_amountcol_name).alias(\"sum\"))\n",
    "        result_win=wingrp_result.select(key2,wingrp_result.window.start.cast(\"string\").alias(\"start\"),wingrp_result.window.end.cast(\"string\").alias(\"end\"), agg_type).toPandas()\n",
    "    if agg_type==\"sum\" and key2_groupby==False:\n",
    "        wingrp_result=data.groupBy(window(date_col_name, frequency_window)).agg(_sum(tran_amountcol_name).alias(\"sum\"))\n",
    "        result_win=wingrp_result.select(wingrp_result.window.start.cast(\"string\").alias(\"start\"),wingrp_result.window.end.cast(\"string\").alias(\"end\"), agg_type).toPandas()\n",
    "    if agg_type==\"avg\"and key2_groupby==False:\n",
    "        wingrp_result=data.groupBy(window(date_col_name,frequency_window)).agg(avg(tran_amountcol_name).alias(\"avg\"))\n",
    "        result_win=wingrp_result.select(wingrp_result.window.start.cast(\"string\").alias(\"start\"),wingrp_result.window.end.cast(\"string\").alias(\"end\"), agg_type).toPandas()\n",
    "    ## Applying date_time function for format conversion of date columns \n",
    "    result_win['end_date']=result_win['end'].apply(date_time)\n",
    "    result_win['start_date']=result_win['start'].apply(date_time)\n",
    "    sort_result_win=result_win.sort_values(by='start_date',ascending=True).reset_index()\n",
    "    ## Making dataframe consisting all dates from start to end of dates present in sort_result_win dataframe to fill in missing dates\n",
    "    time=pd.DataFrame(pd.date_range(start=sort_result_win.start_date[0], end=sort_result_win.start_date[sort_result_win.shape[0]-1],freq=frequency_period),columns=['start_date'])\n",
    "    time['end_date']=pd.DataFrame(pd.date_range(start=sort_result_win.end_date[0], end=sort_result_win.end_date[sort_result_win.shape[0]-1],freq=frequency_period))\n",
    "    # Merging the sort_result_win and time df to get final output\n",
    "    merge_results=pd.merge(time,sort_result_win,how='outer',on=['start_date','end_date'])\n",
    "    merge_results.drop(['end','start','index'],inplace=True,axis=1)\n",
    "    merge_results.fillna(0,inplace=True)\n",
    "    if to_csvopt:\n",
    "        merge_results.to_csv('result_trent_plot.csv',index=False)\n",
    "    return merge_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=get_trend_df(new_data,\"parsed_date\",\"tran_amount\",agg_type=\"avg\",frequency_window=\"2 hour\",key2_groupby=True,key2=\"response\",frequency_period='2h',to_csvopt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>response</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-05-16 00:00:00</td>\n",
       "      <td>2011-05-16 02:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-05-16 00:00:00</td>\n",
       "      <td>2011-05-16 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.246914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-05-16 02:00:00</td>\n",
       "      <td>2011-05-16 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-05-16 04:00:00</td>\n",
       "      <td>2011-05-16 06:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-05-16 06:00:00</td>\n",
       "      <td>2011-05-16 08:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           start_date            end_date  response        avg\n",
       "0 2011-05-16 00:00:00 2011-05-16 02:00:00       1.0  56.750000\n",
       "1 2011-05-16 00:00:00 2011-05-16 02:00:00       0.0  68.246914\n",
       "2 2011-05-16 02:00:00 2011-05-16 04:00:00       0.0   0.000000\n",
       "3 2011-05-16 04:00:00 2011-05-16 06:00:00       0.0   0.000000\n",
       "4 2011-05-16 06:00:00 2011-05-16 08:00:00       0.0   0.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Currently function only supports sum and average\n",
    "\n",
    "NOTE: I did not make a generalised plot function as if key2 if used for grouping and user wants to see a different lines for its key will have atleast 2 or more types e.g if doing a rating analysis group by key rate which has pos ,neg and neutral .Maybe its possible to have many unique legends to be represented so making a generalised plot function would be difficult.\n",
    "\n",
    "### Reference \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=window#pyspark.sql.functions.window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
